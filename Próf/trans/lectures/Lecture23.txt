6:21  
Hello let me today tell you about one last thing related to imperative programming that we didn't touch upon so far. So, we have looked at two simple imperative languages one was called imp which was a very simple imperative language that just has global names and a bit of control flow but nothing like pointers then we looked at micro C, which was the more serious language that on one hand had pointers even to the degree of pointer arithmetic like in like an actual c See also arrays and also this fact that you can use pointers for arrays in the race for pointers like C like in C and that also have functions. And we

7:25  
looked

7:27  
at some of the relevant concepts and principles in particular, we looked at how the interpretation of these languages could go and how compilation could go. And the compilation went with an abstract machine that was based on a stack, specifically a stack that we call the frame stack because for every function call that is currently active, there are some data on the stack that on one hand have to do with computations namely for the arguments for the locals for temporaries resulting from expression evaluation, but then other sort of infrastructure management things having to do with the fact that multiple calls are active at the same time, at one point we will return from the call then we have to return to the to the correct place and we have to discard the data values that were local to the function we just left. So, explain this, these kind of abstract machines are actually very common in the implementation of of languages. So, the the the Microsoft stack machine was the most serious one that we looked in this course it is it is customary these days that for interoperability, you actually define compilers into intermediate languages that could then be virtual machine languages. This is done on one hand both with Java and on the other hand. So the J is the Java Virtual Machine JVM, which is not the physical computer architecture is really an abstract machine for compiling Java, but also other languages too. And a similar infrastructure is is available from Microsoft having to do with the dotnet. It's called the common language infrastructure. And the specific thing is called the Common Language Runtime. So this is all nice. And I advise that you check this chapter nine in the textbook that is specifically dedicated to virtual machines. I won't go through it and I won't explain it in a lecture. It is Sort of a survey like material, it doesn't go into anything in any technical depth, but to know, you know, what different languages used and what technology is available, when it became available, what are the important similarities and differences, this is a very good survey chapter I want to discuss a bit something different now having to do with with data structures that don't have statically fixed size.

10:35  
Like,

10:36  
perhaps linked lists or doubly linked list are three three like data structures that you may want to work with in an imperative language like C. And you do so we only looked at integers, characters, pointers, and well essentially arrays of fixed length, we also have a razor and no pre specified length, but they don't come with any allocation mechanisms. So, these were really essentially you had a possibility to introduce names for arrays, but but not to claim space for there are these data data structures whose size you cannot predict that you want to dynamically allocate space for these are present in C, and in C, you do manual allocation of of space for these things, and manual D allocation.

11:34  
And these

11:35  
data are not stored in the stack, they are stored in another runtime data structure that is called the heap. The heap here is not in the sense of algorithms. But it's in the sense of of programming languages. And it means it's kind of antonym if you wish, to the stack, when the stack is an ordinary thing,

12:00  
you you put

12:02  
things in in a certain order, and you expect them to collect them in in the exact opposite order. So whatever went in last, you can pop first hips are are in contrast to those in that they are a flat, this orderly structure is just some space in memory that is available for you to get to the place data structures in but you manipulate it in a in a fairly free way. And at any stage of your computation, you have some part of the heap used, and the rest are not used, but which parts are used and which are not used. This keeps changing. So let's discuss this topic. So I'll just do a quick overview of the chapter and you can read more details yourself. So I'll share my screen. So we're talking about chapter 10, that talks about heap allocation and and manual heap management versus garbage collection, which means the freeing of the space is done behind your back for you. Okay, so, stack allocation is nice, and you use it for for data with predictable lifetime. And on the other hand, whatever you put on stack, in some sense has has a predictable lifetime, right? Because Surely, things closer to the bottom of the stack have a longer lifetime and things closer to the top have shorter lifetime and if you look at the particular stack position, you can be sure that this stack position doesn't survive longer than the stack the positions beneath it are closer to the bottom of the stack.

14:13  
So, stack is good for predictable lifetimes. So we we work with simple data values of fixed size we can allocate put them on the stack.

14:27  
And we do this as we enter functions and really functions accordingly these things are put on the stack and and discarded. So, in micro C we could use this cell cycle location because several good things work in our favour. But if you have a larger and less disciplined language, all these goodies go away. So so for example, in Microsoft we have the static scope rule. So when you see variable in the programme text. So, the interesting question is always a free variable in the body of a function, then just looking at the programme text, you can figure out where where this variable belongs or you know, which is the associated declaration of the variable. So, for functions, it will always be the declaration from the definition site. And this is a very static notion. So, in contrast with dynamic scope rule, you determine the, the meaning of the free variables in in the functions body based on the function called site and one of the same function can be called from very different places in the programme, and you cannot tell statically this is to say, you know, prior loading the programme to the interpreter or, or prior or during the compilation, you can tell this, right. So, it is we also first order in the sense that we don't allow functions to be returned from functions. And the main thing is Microsoft, sorry, micro c doesn't have these dynamic data structures that are immutable, and whose size you can change during updates to a data structure. So, for example, you've got a list you can you can build a new list by by cleansing cells to the front of it this these are all beta structures with with no static fixed size. Right. So, this is all very good. And micro see in that sense is a discipline language, although, we should say that already in micro C, you can do bad things. So,

16:56  
he,

16:57  
the data are allocated on stack, but because we have this function because the sorry, his language construct called ampersand that returns the address of an of a data value, the address of a location, you can actually already do 30 things there. So, this is mentioned here in this paragraph. So, in micro C, which of course, then happens in accuracy as well, you can write these cool functions that have a local variable, then they declare a local variable, then you obtain an address to that variable. And this will be an address to, you know, to some position in the stack. And then you can, the function can be returning a pointer and in particular, you can return that address to the calling function. Now, when the function returns to the calling function, that frame is discarded. So the position that corresponding to the local variable in particular, is no longer in the stack. I mean, it's in the array that stands for the stack, but it's, but it's, but it's outside, what you actually consider to be the current state of the stack, which are positions between zero and the stack pointer. So then this address is passed out to the calling function, the calling function doesn't know anything, that this corresponded to the address of something that was local to the function that we already left. And then you can do things in the calling function with that pointer. I mean, if the pointer is p, then you can make an assignment to start B, and all of a sudden, you return a value somewhere outside the stack. Okay, this is maybe not so harmful because it's outside the stack. But then you can also read from there and you get, you get garbage values, right? Because the next time the stack grows, who knows which function will write something there, and then you'll just see the value from there. Okay. So already this direct manipulation of pointers, of course, he's dangerous, but it's not the worst thing in Microsoft. But then, most modern programming languages do premier do permit creation of values whose life lifetime you cannot predict. And that is, I mean, it's not tightly title, but it's in particular useful for these dynamically updatable data structures, where you also change the the size and the layout of the data and values with unpredictable lifetime in general and these dynamically sized data in particular, they are stored in this data structure that we call cheap. So for HIPAA look at data, you actually have to reserve space in the heap in the language typically. So here is the syntax for this thing in some languages where this is a thing. So in Pascal, for example, we may want to allocate space for an array of unknown size. Or maybe we want to do allocate space for an array in C or c++. And this goes with typically with malloc, or,

20:37  
or

20:42  
new commands. And here, you actually see that we've allocated some particular amount of space for an array of integers in the case of C and c++. Now in these languages like Pascal and C and c++, when you do not need this dynamically created allocated data structure anymore, then you explicitly have to free the space of it in the heap, you may also not do it, that's always the safe thing to do. But then the danger is your use part of the heap will grow and at one point, you will simply run out of space this is called leaking space. So these languages provide you a command for disposing space in the heap. And these commands are typically called dispose of free. But it is dangerous, because this is a place where a C programmer or a Pascal programmer can very easily slip and this happens very often. Because what you need in deallocation is or freeing space, you need to know the right moment to do it. Okay, so if you never deallocate this is always safe. But this means too much heap space may be used. So that's dangerous in that regard, but it's safe in the regard that you'll never access garbage data by by accident, garbage data arising this way by accident. But otherwise, you may just, you know, deallocate too late, then you're still not using your heap space optimally. And you may run out of the heap space at one point. So that's okay. But what is dangerous is to do okay, too early. So You Think You don't need the data structure anymore. But remember, in these languages, we have pointers. So there is often the phenomenon of aliasing. So P and Q may be aliases, pointers that alias each other, you think you don't need p anymore, because you don't use it in the rest of the programme. But you've created an alias. And this was maybe also with a purpose. And maybe it's called Q and you will later want to dereference q. So now if you free p you also freed Q. And this is this is dangerous. So also all sorts of programming practices that people use to optimise heap usage. But some of them can lead to unexpected side effects if you're not careful. So So this illusion that you manually allocate heap space and you manually deallocate heap space is dangerous, in the sense that allocation is fine. But for the location, on one hand, if you're not aggressive with it, then you're simply leaking speed space, I mean, you're using more space than necessary. But hurrying to the other side, if you if you deallocate too early, this can be really really dangerous. And there is no way for the compiler to find out that what you did was unintended and to warn you I mean, you can write specific special purpose programme analyzers that try to guess your intent based on on certain symptoms and warn you about the overall ideology of course is if, if heap space allocation and de allocation is under programmer control, the programmer is ultimately responsible. So if you've got the freedom, then you also have the responsibility right then And then the possibility to make errors. So a different solution arose actually in the context of functional programming languages, first languages where you may allocate space

25:18  
manually. But collection of space that is actually no longer used is done automatically behind your back. So this, this optimization then of the space used is called garbage collection. So, the idea is data that are there, they perhaps have some meaning, but they're not used in the rest of the computation, they are considered garbage. And the moment that you detect that something has become garbage, you know, the cleaning officer should be able to come and remove the garbage. Now, let's talk about some of these garbage collection techniques. And then I'll show you a particular example of this in action, sort of on a on a on a toy language, which is an extension of this micro C language with with certain features that leads to an extended language that is complicity in the book can then be associated fines that you can

26:23  
play with.

26:24  
Before that, let's talk about garbage collection in general. And then we go to the particular language specification, you have to talk about garbage collection, I have to tell you a bit about the heap. So as I already said, the heap is just an area of memory, some of it is at any given moment of the computation by a programme already in use, or is actively in use. And then there is the rest of the heap that is not currently in use, maybe it was never used, maybe it was taken to use, but then manually do okay. Or maybe that garbage collection, collector feed it again. And now it's free. Yeah, so in the heap, you talk about the use part. And the free part. And there is typically some sort of a data structure for you, there available to talk about which part of the heap is free, because if you look at the heap, then it's just an area of memory. And that every address, there are some values, but you don't know which values are the real data, and which are just some data left there that actually are not currently in use. I mean, you may, of course, want to reserve a bit in every word to tell you whether that piece of data is currently in use or not. This is sometimes done, and it's actually used by some of the garbage collection algorithms as well. But in general, if you just look at a piece of memory, this is Yeah, it's, it's neutral matter. I mean, it's, it's just words, you have no idea what these words stand for. So and then to overcome this, we work with a thing called freeze. Actually, let me introduce some some more terminology before. So typically, when we talk about garbage collection, we have the situation that there is, in some sense two programmes acting at the same time, on the same memory, so there is your programme of interest, a compiled machine code that does some interesting computation. This one is called the mutator. It's, it's, it's the programme that manipulates the stack and it also manipulates that he changes. values there, it allocates space for for new values, updates all values. The programme of interest in this context is often called the new ticker. So if nothing else, then your programme combined. There is then usually another actor in play that is working in parallel to your programme. It's sometimes sometimes sort of busy all of the time, but more More, more typically, it's activated now and then and it's called collector. So this is the garbage collector. It inspects, what the mute mutator is doing. And based on how the heap develops now and then it becomes active and reclaims unused space. Now, the space as I said comes into part that is the use part and the unused part. The unused part is typically kept in some sort of a data structure. The most common One data structure is called the free list. So what is the free list is literally a linked list of all the following things. The free list is a linked list of blocks, chunks of the heap that are not currently in use. So what is an element in such a list? Well, it is really a pair of two things. So you really need to know about you need to work with these blocks, right? Every unused block will will have a header, which is the size of that block. So block knows how big it is. So if you learn that the header of a block, there is the information about how big the block is. And then also, there is a pointer to the next unused

31:09  
block

31:10  
on the free list. So a free list for every block knows the address, the start address of the block. And then the size of the block. And this is how your memory is organised. In the end, all you need to know to know the free list is to have a pointer to the first block on the freeness. So this is some address in the heap. There you also see the size of the block and then the next of the rest of the block is next to it right. Now, this is what a free list is it's just a linked list of blocks and the block is identified by its start address and its size. Now, when you allocate new memory for a new data structure, by your programme, then you need for it a block of a sufficiently big size. And this is done how you just traverse the free list. And you'll find the first block that has the appropriate size, you take that block into the use into use. I mean, you take as much as you need for your data structure, the rest of the block is a smaller free block. And you've just updated your your free list, add that element by sort of changing the size. Now, of course, if you need bigger blocks, then you may need to concatenate several blocks into one. And if your heap has become very discontinuous very discontiguous, then then you may need to merge blocks together. And that's also done. But

33:14  
let me let me not explain this here. Now. I mentioned another thing. So these blocks are the free list approach has the problem that sometimes the first

33:26  
block of large enough size can be much too far down deep in the list. The better approach that is also sometimes used is that you keep a separate with the free list of blocks of different sizes. So maybe, maybe basic blocks are of some size K, but then you have a different list for blocks of size 2k at least then the yet different list for blocks of size 4k at least and yet different list of blocks of size 8k at least. And then depending on in the allocation, how big a block you need, you just extract your go to the appropriate fee list and you take the first block from there, then of course when you when you free blocks, it may happen that that again, you have a bigger block available, I mean, maybe you had a block broken into two where the first part was used. And the second wasn't. Maybe the second one is is reclaimed by the garbage collection, then at this point, you can merge two blocks of half size together into one block of the full size again and put, you know into the right place in this in this organised list of lists. So this much is important for you to know about, about the free list. So it's a way to keep record of which areas of the heap are free. But now, to these garbage collections, garbage collection algorithms per se, there are three classical ones. And it's important that you know, just the principles of each, maybe not the details. So one is called reference counting. And one is called mark and sweep. And yet another one is based on copying half spaces. And I'll explain each of the three. In turn, very often, you use something that is called generational garbage collection, which means that you distinguish between older and newer data in the heap. And the idea being that

36:04  
the data that have been there already for a long time with with high likelihood stay there even longer. And then the data that have been there for a short time. You know, initially, they have a high probability of being short limb that they don't survive for long. And then based on this, you divide data into generations. And you actually use may use different garbage collection methods on different generations. That's also very common. generalisation or sort of strategies, right. Okay. Let me briefly mentioned the ideas of each of the three. And I don't go deep into any of these you just read in the book, that's my recommendation. So the idea of reference counting is that for every piece of data in your heat for every cell, that you you keep track of how many pointers from the stack currently points to it or how many indirect pointing there is from already the stack to the heap and from the heap further to heap so. So you count all references both from the heap of both from the stack and to the heap, for given a cell to a given object on the heap. And, and the mechanism is very simple. So each time an assignment happens, you just you just increment or decrement, the reference count. So whenever an object is created, then nothing refers to it. But then of course, when an object is assigned to a pointer, then then already if a new object is assigned to a pointer, then they count is incremented. To one because there is now one reference to the object. When the mutator performs some, some assignment of the form x equals now. So x pointed to something. But now it is a null pointer to think of x is a pointer, then the reference count of the object previously referred to by x has to be has to be decrement. It I mean, it's it doesn't necessarily become zero, because there can be other pointers to the same object. But but at least it diminishes by one, this is the main mechanism by which these counts, ever go down his assignments, and here's the general assignment where one reference goes up, and then other goes down. So suppose we perform an assignment x equals Oh. So in this case, well, x pointed previously to something, the reference count of that object must be decreased by one, but x now will point to also the reference count of the object, our must increase by one then the idea of reference counting is that whenever the reference count of an object actually reaches zero, then the object may be de allocated, which is to say it can be put on the field list. Moreover, at the moment you do this, you can also Dec decrement, the reference count, so every object that pointed to sorry, so it's always a structured object, like it has fields, so then the counters of the fields must also be decremented. So if ofoh itself points to something like an object points to its fields, then the reference counts of those are of course also decrement. It's basically if you The idea is if you, if you remove a structure thing, like if you remove a list because that has fields hadn t. So if you if you remove this first console, then then surely the reference count of the tail also gets decrement, right. And maybe if that one becomes zero, then you'll remove its tail as well. So this is an algorithm that is sort of intuitive, and it's simply simple to implement. One thing that you notice is that there is some overhead both in space and time, namely, next to every object, you also need memory to hold its reference count.

40:50  
So, an object really becomes what it means some in some sort of a cell. But now, there is an extra extra extra field there, which is the objects reference count. But then also, basically, you have to babysit all of the all of the execution by the programme by the mutator, because at every assignment, you have to do this manipulation of counters, there is a further disadvantage, which is that you may have cyclic data structures like a circular list. And in a circular list, there is always already at one I mean, the reference count in a circular list of every cell is at least one because you know every cell is pointed by pointed to by some other in the circle. So you may have had some pointer to the circular list, maybe this one becomes now. Okay, so then the most that can happen is is that the reference count or what was previously the head decreases from two to one, but it never never decreases to zero. So with reference counting, you will never ever deallocate anything circular. So that's also one of the disadvantage. It's also mentioned here. Another way of doing garbage collection is called market sweep. So the idea here is, is an entirely different strategy. It's not that the collector is busy all of the time and sort of watching, watching by the mutator over its shoulder and each time the mutator does some assignment, you know, some some additional steps are made by that garbage collection, nothing collector, nothing like this. So Mark can sweep, the collection is activated when the programme is almost out of the heap space. So you're close to the situation where you cannot allocate the next data structure, you need to according to the programme, so then computation stops, the garbage collector takes over and carries out this thing called mark and sweep. And it does the following. So basically, you you start from something that is called the root set. I mean, everything that is used in the heap is ultimately via indirection pointing to from the stack. Yeah. So in the stack, you have some pointers to the heap, and then further point, there's within the heap. So what do you do, you start the following kind of a mark procedure, and that also requires that all of your words contain at least you know, one bit, or, in the case of some improvements, further bits, in, in their words reserved for for the for the algorithm to work that you don't use for data that is but it's just used for this bookkeeping. So you so first to make all of the heap cells, sorry, all of the heap. Yeah, all of your heap objects on unmark. Then your, your mark every object in the root set. So these are the objects directly referred to but from the stack, you mark them as in use. It's basically a set Bit to be true, then you look at who they point to recursively and mark those as use when you look at who the next generation points to recursively and set these as used as well. And then hopefully, you'll convert I need to comment on this because it's not trivial. And then everything that is actually in use has been marked as used and then you traverse once this is done. So, this mark, this place is called marking. Once you're done with this, you start another face that is called sweeping you just walk through the heap and for every object that you discover is still marked unused, you reclaim you move it to the free list.

45:51  
Yeah. Now about marking specifically how do you do this, I mean, surely, you can go over the stack and see about the root set that part is clear, but then the rest of the recursion if you do the recursion in a depth first manner, then of course, when you have cycles, then you can just get stuck in the cycle you go round and round and round and you mark the same things over and over and over and over. So that is not a good strategy. A better strategy is you you just traverse the heap once and for every object in the heap, you find you also mark all objects refer to it as used in the first traversal not everything gets marked as used. So if Yeah, if something is used, then whatever is further referred to from a used object to mark is used. So in the first phase, of course, you only capture the second generation, but then if you do it multiple times, then you're basically done mark and sweep in a kind of a breadth first fashion just by just by multiplying multiple times traversing the I mean in this correct way, it is fairly simple to implement.

47:34  
The disadvantage of course, is I mean, this is a cyclic thing, you only do it now and then when the heap is about full use, and then it takes quite a bit of time because you have to sweep potentially the entire heap many times before, before you've marked everything that is used.

47:56  
Okay.

48:04  
And then there is this stop and copy which uses two half spaces or two spaces. So, the idea is you split the actually heap into two parts, or the the the memory of what available for the heap into two parts do exactly the same size halves and then you do something that sort of sounds quite waist full. Namely, only take one half into the use. Yeah, and you work with this one, huh, when this one half is almost full, you have you start this top and copy collection, then the idea is again, you start from the root set, and you just copy over things, the use things from the

49:04  
from the active half space to the other half space. And you do it in the same way by by sort of multiple sweeps. Or you can also do it by recursion now, because whatever you discover, as used you at the same time, move over to the other half space. So the the idea is is a bit like in Mark and sweep but it's not done in place, the moment you you you discover something is used you move it over to the other space and the way that you do it of course then you have to adapt all the references is that the the other space is used contiguously Yeah. And then once you've done that with a copying the the the space was that was currently active, it's called the from space. This becomes the passive I have space that is then called the to space and tools and what was previously the to space becomes the from space. And this is a nice algorithm, but but again the you know, the disadvantages are sort of similar to market sweep. So, it takes time and advantages that you do compaction. So, you do not get unused unused blocks between the use blocks. So, this this is a systematic way of avoiding fragmentation and also why you do this you already fixed you know the point there. So, they point to the right place kind of in a in a systematic way. So, there is no additional overhead from that. The one is what are the disadvantages, one big one is you can only use half of your available memory space for life data.

51:12  
And this, one thing that gets especially critical also is there is this copying involved that suppose you work such that you have before a lot of time, almost all of the space used. So, because you're you're you're close to the half space being full, you copy things over many and many times to do very little compaction, perhaps, but but you just copy over the whole thing. Okay, so these were the three main and these are sort of important to remember, on the conceptual level, read here in the book, look at the figures make it very clear to yourself.

52:07  
Now,

52:08  
let me finish with what is also done here. There is an explanation of how to do this for for in our C like setting. And this is done on a on a simple extension to the micro C language that gets nowhere close to full C. But at least it's something right. And it illustrates a real garbage collection in action. This is the nice piece about it. So let's see what it does. It gives you essentially the infrastructure for doing singly linked lists. But you can also use it for W linked lists, trees, other kind of dynamic, dynamically sized data structures. So essentially, we can allocate Khan's cells and we have a pipe dynamic specifically for these kinds of things. So let me explain this a bit. So here is the syntax, it doesn't have a lot. So there is an additional type dynamic in addition to the char and pointers and arrays. And any value in dynamic can be a value or a reference

53:34  
to a console or Neil and then you have these kinds of expressions. So Neil is a null reference. So it's it's, it's a reference to a specific value that you can observe the values that you recognise that you can recognise it's not I mean like an invalid invalid value. So, then there is an expression cons e one e two v one and D two are expressions. And when you evaluate this, then actually a new cons cell is allocated in the heap and its components, it has two components v one v two think of them as two fields and they are exactly the fields for v one and v two. So, if you want to do lists, then actually you will take the first one of them as the integer as an integer data value and you will think of the other value as a as an address to the next console which is the next thing in your linked list. Then, if he is an expression referring to a console, then car gives the first component is singer gives the second component and these are sort of historical names from lists and related like Images. And then there is so this is for reading the two components of Excel. And then also we have means to update the first or the second component of the cells self care sets. P is the pointer is the name the expression whose value should be a point? Yeah, should be a reference to the cell. And we should be the actual data value that you want to put there, which may also be a pointer because here, there is no No, no type checking going in. Right? So, maybe I'll illustrate this a bit. So how do you do lists with these.

55:46  
So, the first examples actually do not use this mechanism for lists, they are just pairs allocated in the heap. But then, for the next ones, we actually do the real list. So, so here's a programme, the main programme returns nothing, of course, this main should, and it takes one argument. And the idea is, in this particular case, we don't actually use the end at all, but in the next one to use so. So there is a local variable C, that we declare is dynamic. When we say C equals cons, at this point, a console is allocated on the heap and C gets C is, is the pointer to it. Then we when we print car, actually 11 is printing then when we print sitter 33 is printed. So here, there is another uninteresting example. But let's look at more interesting ones.

56:51  
So wait, so here, we've actually used using the fact that you can use these concepts to build the list of integers by, for example, in just by storing an integer in the first cell, and in the second and address or an integer that you view as an address. So here is the function print list, it takes x is of type dynamic. So that's a pointer to a console. And then while this is not now, you print the first element, and then you update the pointer to be the address in the second cell. Right, so axes, cover axes. So if you do this, then of course, you repeatedly print the head of the list until you reach the end, which you recognise, because that's a null pointer. And here are some more functions for for for making lists, for example, we can make a list of numbers one through m in this way. So So now it's a function that takes an integer and produces a dynamic. So how does he do so we first have a local dynamic named rests, which is actually passed out. But it's heap allocated to that, okay, so we're not passing out a pointer to anything in the stack. And then we say, initially, rest is nil. And that is the null pointer. So this corresponds to us being at the end of the list. And then we sort of keep changing rest, two pointers to more interesting lists. So while n is greater than zero, we create a new cell where we put the head component to have the value n. And then the second component corresponding to the tail is in this reference race, and we get the new reference rest. And then we decrement. And And finally, when we return race, it points to the very beginning of this history list, which was created from all these courses. And then you have other examples, which I recommend you've gone through yourself.

59:13  
Now,

59:17  
how to work with this, well, let's talk about the compiler. So, we had a compiler producing byte code for a stack machine. Now we just add a few instructions so that we get the stack and the heap machine. And then we can think, how to execute this machine sorry, how to implement this machine, while in the usual way, such that it also comes with the associated garbage collector. So here it's done actually with Mark and sweep. So what would we have In the instruction set for these abstract machine manipulating both the stack and the heap, you have these extra instructions that more or less directly correspond to those from the higher level language from lists see. So, mail actually pushes the needle reference to the, to the top of the stack. So it's a reference to the designated now value when I want to create the cons, so then I've got these v1 and v2, which can be integers or addresses. I mean, this thing doesn't know I mean, there is no difference between these two things. But anyway, so that's the stack after stack before the stack after the executing this instruction will be whatever the stack was as part was before. But then the top element is now p, it's an address to the heap. It's not it's not a stack address is a heap address corresponding then to the first position of this data structure that was actually allocated in the heap. So, so this is allocated in the heap. And the free list is is is correspondingly updated. I mean, the, we go to the, to the first block there, and we shrink it size by however much is required for for this guy. Current singer, they take this address to the heap, and they look up the values there. Set Karthik and others to the heap and the value and they go to the heap and update the heap. And so what is returned is the same s&p but the actual heap has changed. And same for sets. Now, let's now talk about the

1:02:15  
the

1:02:19  
the abstract machine and its collector part. So previously, I was a bit vague about what are these objects in a heap? I mean, what actually makes an object in the memory and how on earth? Memory is just the sequence of words, right, but how should the garbage collector in general know, you know, which words together constitute some sort of object, of course, you expect some sort of structure on these heap objects. So here, this is done in a very simple way. So we actually, the heap just consists of words. And the words are for integers. The words are 32 bits, but here we need a tiny bit for our bookkeeping. So the integers are actually 31 bit. So the 32nd bit is used for for bookkeeping. Yeah. And then what we do is we just distinguish addresses, I mean, things that are to be understood that addresses from those that are to be understood as as as as integers. So an integer will have the designated bit as one, and the reference will have a designated bit the zero, which is a very simple a naive thing to do. Of course, there is a cost namely, you can't use the native 32 bit integer instructions anymore that you may have, from your underlying architecture to actually implement your machine. You need 31 bit operations and you need to make sure that your operations don't touch these, these designated bits, but just remember if you've got addresses or or integers, but that can be done right. So you have this, but then how do these How do these cells look like in the heat that is also important? And it's the following things. So so so basically, the cell consists of three parts I would say there is a there is a header which is 32 bits, and that is the first field which is 32 bits, and there is a second field. So these are called car and sitter. And these are also 32 bits. So, so basically your heap memory looks like this. So, every block is a header colour code your header The next one is header car cover header car control here this mistake

1:05:21  
okay um but then yeah okay, but I should say. So, there are the regular blocks corresponding tu tu tu tu sells two consoles, then there can be of course, other blocks and maybe the picture it was three illustrate this. So, in an another block is just some block from the free list right and these can be arbitrary long they also have a header Now, what is the header the header is 32 bits 32 bits organised in some way There are eight tag bits and for a console the tag bits are all zero. So, the T's are all zeros

1:06:20  
and then there is the length the length part the length part is the one that is important for four blocks in the field just because you need to know how long they are. So, here you use a 22 bit integer to indicate the length then of course, as cellblock in particular consists of two useful words and the first is the header word. So, for that one is 000 10 standing for the binary number two, but for a general block this can be any 22 bit integer number and finally various two more bits that are used during the market sweet and not otherwise.

1:07:11  
Yeah

1:07:31  
the yeah the garbage bits to actually stand for colours 00 means white 01 means grey 10 means black. So these are colours used in the in the market sweep and 11 it means blue. And that is a particular colour for things that you know are in the free list. So Mark and sweep doesn't even need to inspect them. Okay. So how does how does how does a freeze block like it has it has a header exactly like this and then it has these other blocks but actually the first one of them is in a special use because the free list has to be a linked list. So for a block in the free list of course we have to know if it's the last block in the free list. So this is indicated by a word that is all zeros. But it can also be not all zeros and in this case it's just the address of the next block on the free list. Yeah, so I mean if this this block here is a block from the free list when the actual free blocks are these two and this one is a special word which if it's all zeros indicates that this is the last block in the free list. If it is some other number, then it is the address of the next block in the free list. Yeah, and and this this header is also like for these console blocks. Okay. Now that is all good. How does mark and sweep work So let me explain it in two stages. So first only using three colours white, black and blue, and then I can explain, also the grey. And the explanation is here, and it's sort of corresponds to the general explanation. So in the mark, you'll go, you'll go through the programme stack and you find all references into the heap. And how do you do that? So? So first of all, you have to understand that the system is weakly typed, right? I mean, we give things types, but actually, we use things as integers or addresses interchangeably. But now, for the garbage collector to work properly, of course, we need to know whether something is an integer or, or is an address, right? difficut a value on the stack. And if it is an integer, then it's just the data value and then it doesn't correspond to anything that would be an address on the heap. So we shouldn't just look at this number and treat it as an address and say, Okay, yeah, this position in the heap is in use, because no is the database. So So value in the stack is either either an integer that is like a normal data value, or maybe a stack address. Or it's a heap reference. And then this was distinguished by by a bit in all these words, right. So it's a heap reference, when when we encounter such a thing, and we see it is a reference to a white block, white stands for unused, then we mark it black, it means it is used. And then you can recursively process all of the all of the blocks words in the same way. So the block, again contains words. And depending on what their nature ease, if they are sort of data values, or if they are addresses to the heap, you have to process them.

1:12:15  
And

1:12:16  
this is the mark mark face. And then, as a result of this mark trace, every block in the heap is either black, because this means it was recursively reachable from the stack by a sequence of references. But it's white, which means it's not reachable. This means it's unused. Or maybe it's blue. Because it's already or what was on the free list all the time. And then in the sweet face, of course, you just traverse all blocks on the heap. If a block is white, this means it was a block that was not in use, we can paint it blue, and that to the free means if a block is black. This means it was in use. But then of course, we can reset it to white. Because the next time when we want to do the mark, again, we want that all of these are white. So White is kind of the default colour right before garbage collection, everything has to be white or blue. So not currently marked or in the fields. So that's possible, but there is an improvement that goes by by these grey things. So this doesn't work then IE depth first recursion by this more clever thing that I already referred to before is with the better Mark face. So let me explain this.

1:13:51  
So, it goes into faces. So, when you find a block that is referenced from this stack, then the corresponding block in the heap is painted grey, not black directly. So now, when you have to traverse the stack, then all blocks that are directly reachable from the stack they are already grey and nothing is black. Then when you traverse the heap, then we may find as this goes in recursion, we may find no white grey or black blocks. If we find a grey block, then we mark the block itself black, which means we've actually dealt with it and then we look at the words in the block. And if they contain the reference to the white block, then we mark it

1:14:47  
grey.

1:14:51  
Then the next time when we come back some of the blocks already marked black then you don't Go through there through these blocks again, to look at further references, we know, black means it was analysed already, we don't need to touch it anymore. This is done. And we only repeat the process for grey blocks. So the idea is grey are blocks that are in use. But from which we haven't yet analysed the further references, black blocks that are in use, but we've already dealt with references from that block further on as well to the finer form of

1:15:49  
bookkeeping.

1:15:50  
So eventually, when the mark face is done, everything is white, blue or black, then you do the sweep sweep gets rid of the black and everything is white or blue. Again. Okay, so that was the explanation. I encourage you to go through this independently read everything in detail with the lecture attendance here is not great. I was listening for this alone. So this doesn't motivate me to prepare slides. I've read the book. I also encourage you to read the book read the blue read the book, also check the list see abstract syntax and the compilation to understand what is going on and to check the garbage collection collector. And and then in the last lecture I'm I mentioned I go through the rest of the last lecture. I'll revisit some topics and we discussed we discussed the format of the exam.

1:17:13  
Okay, bye

Transcribed by https://otter.ai
